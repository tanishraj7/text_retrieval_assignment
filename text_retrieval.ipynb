{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ccf5b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# Create a sample dataset in JSON format\n",
    "data_dir = \"datasets\"\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "# Sample dataset to simulate the Q&A setup\n",
    "sample_data = {\n",
    "    \"corpus\": {\n",
    "        \"doc1\": \"What is artificial intelligence?\",\n",
    "        \"doc2\": \"Explain machine learning and its applications.\",\n",
    "        \"doc3\": \"How does deep learning differ from traditional machine learning?\"\n",
    "    },\n",
    "    \"queries\": [\n",
    "        \"What is AI?\",\n",
    "        \"What are machine learning applications?\",\n",
    "        \"Difference between deep learning and machine learning\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Save sample data\n",
    "dataset_path = os.path.join(data_dir, \"fiqa.json\")\n",
    "with open(dataset_path, \"w\") as f:\n",
    "    json.dump(sample_data, f)\n",
    "\n",
    "# Function to load dataset\n",
    "def load_dataset(data_dir, dataset):\n",
    "    with open(os.path.join(data_dir, dataset), \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    corpus = data[\"corpus\"]\n",
    "    queries = data[\"queries\"]\n",
    "    return corpus, queries\n",
    "\n",
    "corpus, queries = load_dataset(data_dir, \"fiqa.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3b2d779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence-transformers\n",
      "  Downloading sentence_transformers-3.1.1-py3-none-any.whl (245 kB)\n",
      "     ------------------------------------ 245.3/245.3 kB 791.4 kB/s eta 0:00:00\n",
      "Collecting huggingface-hub>=0.19.3\n",
      "  Downloading huggingface_hub-0.25.1-py3-none-any.whl (436 kB)\n",
      "     -------------------------------------- 436.4/436.4 kB 2.3 MB/s eta 0:00:00\n",
      "Collecting transformers<5.0.0,>=4.38.0\n",
      "  Downloading transformers-4.45.1-py3-none-any.whl (9.9 MB)\n",
      "     ---------------------------------------- 9.9/9.9 MB 1.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: tqdm in c:\\users\\tanish raj singh\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.64.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\tanish raj singh\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.12.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\tanish raj singh\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.10.0)\n",
      "Requirement already satisfied: Pillow in c:\\users\\tanish raj singh\\anaconda3\\lib\\site-packages (from sentence-transformers) (9.4.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\tanish raj singh\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.2.1)\n",
      "Requirement already satisfied: requests in c:\\users\\tanish raj singh\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.19.3->sentence-transformers) (2.28.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\tanish raj singh\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.19.3->sentence-transformers) (6.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\tanish raj singh\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.19.3->sentence-transformers) (3.9.0)\n",
      "Collecting fsspec>=2023.5.0\n",
      "  Downloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
      "     -------------------------------------- 179.3/179.3 kB 1.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\tanish raj singh\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.19.3->sentence-transformers) (4.4.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\tanish raj singh\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.19.3->sentence-transformers) (22.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\tanish raj singh\\anaconda3\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\tanish raj singh\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers) (2022.7.9)\n",
      "Collecting safetensors>=0.4.1\n",
      "  Downloading safetensors-0.4.5-cp310-none-win_amd64.whl (285 kB)\n",
      "     -------------------------------------- 285.9/285.9 kB 2.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\tanish raj singh\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers) (1.23.5)\n",
      "Collecting tokenizers<0.21,>=0.20\n",
      "  Downloading tokenizers-0.20.0-cp310-none-win_amd64.whl (2.3 MB)\n",
      "     ---------------------------------------- 2.3/2.3 MB 1.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\tanish raj singh\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.1.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\tanish raj singh\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (2.2.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\tanish raj singh\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.19.3->sentence-transformers) (1.26.14)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\tanish raj singh\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.19.3->sentence-transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\tanish raj singh\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.19.3->sentence-transformers) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\tanish raj singh\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.19.3->sentence-transformers) (2022.12.7)\n",
      "Installing collected packages: safetensors, fsspec, huggingface-hub, tokenizers, transformers, sentence-transformers\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2022.11.0\n",
      "    Uninstalling fsspec-2022.11.0:\n",
      "      Successfully uninstalled fsspec-2022.11.0\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.10.1\n",
      "    Uninstalling huggingface-hub-0.10.1:\n",
      "      Successfully uninstalled huggingface-hub-0.10.1\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.11.4\n",
      "    Uninstalling tokenizers-0.11.4:\n",
      "      Successfully uninstalled tokenizers-0.11.4\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.24.0\n",
      "    Uninstalling transformers-4.24.0:\n",
      "      Successfully uninstalled transformers-4.24.0\n",
      "Successfully installed fsspec-2024.9.0 huggingface-hub-0.25.1 safetensors-0.4.5 sentence-transformers-3.1.1 tokenizers-0.20.0 transformers-4.45.1\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence-transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4075a20a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e872acd01b94be9a991217075d62ab5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tanish Raj Singh\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Tanish Raj Singh\\.cache\\huggingface\\hub\\models--sentence-transformers--all-distilroberta-v1. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b68b7dcca3e4a688448ddd74de3cd6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "beb56abd7d57400da912c9af3d5fbc1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "298be0dc547548ee990698e8fcef23e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e03e97ac73444be98baccb289a13a152",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/653 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23bd8d04a0954d4494839b5a3bfe7fac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/328M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6979be1f478b43bf87b724067f7696c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/333 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23c802cfe266418c93fb4aa6d5f4fa65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38ae0a0575e14283aa7c3d50fa188f6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53f7ea9a716940e89971be7e4fe177ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "565b295d87224007bf342a87aed0c361",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbe76f6599e94567ad52c3aac08be8bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings using the small model:\n",
      "tensor([[ 0.0439,  0.0589,  0.0482,  ...,  0.0522,  0.0561,  0.1021],\n",
      "        [-0.0169, -0.0707,  0.0855,  ...,  0.0989,  0.0128, -0.0849],\n",
      "        [-0.0727, -0.0169,  0.0228,  ...,  0.0767,  0.0406,  0.0370]])\n",
      "\n",
      "Embeddings using the large model:\n",
      "tensor([[-0.0321,  0.0386, -0.0312,  ...,  0.0085, -0.0138,  0.0431],\n",
      "        [-0.0031, -0.0556, -0.0267,  ...,  0.0325, -0.0649, -0.0093],\n",
      "        [ 0.0010, -0.0345, -0.0188,  ..., -0.0308, -0.0089,  0.0098]])\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load embedding models\n",
    "embedding_model_small = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "embedding_model_large = SentenceTransformer('sentence-transformers/all-distilroberta-v1')\n",
    "\n",
    "# Example corpus\n",
    "corpus = {\n",
    "    1: \"The quick brown fox jumps over the lazy dog.\",\n",
    "    2: \"I love machine learning.\",\n",
    "    3: \"Transformers are very powerful for NLP tasks.\"\n",
    "}\n",
    "\n",
    "# Encode corpus using embedding models\n",
    "corpus_embeddings_small = embedding_model_small.encode(list(corpus.values()), convert_to_tensor=True)\n",
    "corpus_embeddings_large = embedding_model_large.encode(list(corpus.values()), convert_to_tensor=True)\n",
    "\n",
    "print(\"Embeddings using the small model:\")\n",
    "print(corpus_embeddings_small)\n",
    "\n",
    "print(\"\\nEmbeddings using the large model:\")\n",
    "print(corpus_embeddings_large)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46e7b621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reranked results using the small model: [[(0, 8.202035903930664), (3, 6.538022041320801)], [(1, 8.006841659545898), (2, -6.739014625549316)]]\n",
      "Reranked results using the large model: [[(0, 9.328752517700195), (3, 8.882637977600098)], [(1, 9.346213340759277), (2, -10.456615447998047)]]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Load cross-encoder reranking models\n",
    "reranker_small = AutoModelForSequenceClassification.from_pretrained('cross-encoder/ms-marco-MiniLM-L-12-v2')\n",
    "reranker_large = AutoModelForSequenceClassification.from_pretrained('cross-encoder/ms-marco-TinyBERT-L-2-v2')\n",
    "\n",
    "tokenizer_small = AutoTokenizer.from_pretrained('cross-encoder/ms-marco-MiniLM-L-12-v2')\n",
    "tokenizer_large = AutoTokenizer.from_pretrained('cross-encoder/ms-marco-TinyBERT-L-2-v2')\n",
    "\n",
    "# Sample corpus and queries\n",
    "corpus = {\n",
    "    'doc1': \"This is the first document.\",\n",
    "    'doc2': \"This document is the second document.\",\n",
    "    'doc3': \"And this is the third one.\",\n",
    "    'doc4': \"Is this the first document?\"\n",
    "}\n",
    "queries = [\"first document\", \"second document\"]\n",
    "\n",
    "# Simulated retrieval function (replace this with your actual embedding retrieval)\n",
    "def simulate_retrieval(corpus, queries):\n",
    "    # Simulating retrieval by returning document IDs for each query\n",
    "    # Each query gets the top 2 documents (for demonstration)\n",
    "    return [\n",
    "        [{'corpus_id': 0}, {'corpus_id': 3}],  # For \"first document\"\n",
    "        [{'corpus_id': 1}, {'corpus_id': 2}]   # For \"second document\"\n",
    "    ]\n",
    "\n",
    "# Step 1: Candidate Retrieval\n",
    "retrieved_small = simulate_retrieval(corpus, queries)\n",
    "retrieved_large = simulate_retrieval(corpus, queries)\n",
    "\n",
    "def rerank(corpus, queries, retrieved, reranker, tokenizer):\n",
    "    reranked_results = []\n",
    "    \n",
    "    for query, hits in zip(queries, retrieved):\n",
    "        reranked_scores = []\n",
    "        for hit in hits:  # Each hit has 'corpus_id'\n",
    "            passage = corpus[f'doc{hit[\"corpus_id\"] + 1}']  # Corpus starts with doc1\n",
    "            inputs = tokenizer(query, passage, return_tensors=\"pt\", truncation=True)\n",
    "            outputs = reranker(**inputs)\n",
    "            score = outputs.logits.item()\n",
    "            reranked_scores.append((hit['corpus_id'], score))\n",
    "        \n",
    "        # Sort based on score\n",
    "        reranked_results.append(sorted(reranked_scores, key=lambda x: x[1], reverse=True))\n",
    "    \n",
    "    return reranked_results\n",
    "\n",
    "# Step 2: Rerank using small and large models\n",
    "reranked_small = rerank(corpus, queries, retrieved_small, reranker_small, tokenizer_small)\n",
    "reranked_large = rerank(corpus, queries, retrieved_large, reranker_large, tokenizer_large)\n",
    "\n",
    "print(\"Reranked results using the small model:\", reranked_small)\n",
    "print(\"Reranked results using the large model:\", reranked_large)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "48c493c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG@10 (Small Model): 1.0\n",
      "NDCG@10 (Large Model): 1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Calculate NDCG@10\n",
    "def calculate_ndcg(reranked_results, true_relevant_docs, k=10):\n",
    "    def dcg(relevances):\n",
    "        return sum([rel / np.log2(idx + 2) for idx, rel in enumerate(relevances)])\n",
    "    \n",
    "    def ideal_dcg(relevant_docs):\n",
    "        return dcg([1] * min(k, len(relevant_docs)))\n",
    "    \n",
    "    ndcg_scores = []\n",
    "    for i, reranked in enumerate(reranked_results):\n",
    "        relevances = [1 if doc_id in true_relevant_docs[i] else 0 for doc_id, _ in reranked[:k]]\n",
    "        ndcg_scores.append(dcg(relevances) / ideal_dcg(true_relevant_docs[i]))\n",
    "    \n",
    "    return np.mean(ndcg_scores)\n",
    "\n",
    "# Assuming true_relevant_docs is the ground truth mapping\n",
    "true_relevant_docs = [\n",
    "    [0],  # For \"What is AI?\" -> doc1 is relevant\n",
    "    [1],  # For \"What are ML applications?\" -> doc2 is relevant\n",
    "    [2]   # For \"Difference between DL and ML?\" -> doc3 is relevant\n",
    "]\n",
    "\n",
    "ndcg_small = calculate_ndcg(reranked_small, true_relevant_docs, k=10)\n",
    "ndcg_large = calculate_ndcg(reranked_large, true_relevant_docs, k=10)\n",
    "\n",
    "print(f\"NDCG@10 (Small Model): {ndcg_small}\")\n",
    "print(f\"NDCG@10 (Large Model): {ndcg_large}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a910112",
   "metadata": {},
   "outputs": [],
   "source": [
    "#text_retrieval_assignment"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
